{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f439285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, datetime\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unidecode\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LinearRegression, Lasso\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141e52f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca78b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SentimentLabeled_10112022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b333fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select tweets about China only\n",
    "df = df[df['country']=='China']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3385ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'id', 'Bucket', 'SentimentScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b95ce",
   "metadata": {},
   "source": [
    "### Bucket Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b63994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Remove tweets that do not have sentiment score\n",
    "#Step 2: Average the sentiment score for each unique tweet\n",
    "sent_df = df.copy()[['text', 'id', 'SentimentScore']]\n",
    "sent_df.dropna(subset=['SentimentScore'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8fb8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>SentimentScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#NorthKorea is propped up by regimes like Chi...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"... If China had learned the lessons of Tiena...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"...Jim Banks, an Indiana Republican, slammed ...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Accelerating progress in vaccinating people, ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Although the U.S. is currently ahead of China...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"American Needs to Invest in Future Tech *Now*...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"America’s foremost nat-sec threat is China......</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"An important new report from Sen. Marco Rubio...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Anyone hoping that China is finally turning a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Between the sanctions, diplomacy &amp;amp; having...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  SentimentScore\n",
       "0  \"#NorthKorea is propped up by regimes like Chi...             2.0\n",
       "1  \"... If China had learned the lessons of Tiena...             1.0\n",
       "2  \"...Jim Banks, an Indiana Republican, slammed ...             1.5\n",
       "3  \"Accelerating progress in vaccinating people, ...             2.0\n",
       "4  \"Although the U.S. is currently ahead of China...             2.0\n",
       "5  \"American Needs to Invest in Future Tech *Now*...             2.0\n",
       "6  \"America’s foremost nat-sec threat is China......             1.0\n",
       "7  \"An important new report from Sen. Marco Rubio...             2.0\n",
       "8  \"Anyone hoping that China is finally turning a...             1.0\n",
       "9  \"Between the sanctions, diplomacy &amp; having...             4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df = pd.DataFrame(sent_df.groupby(['text', 'id'])['SentimentScore'].mean())\n",
    "sent_df.reset_index(inplace=True)\n",
    "sent_df = sent_df[['text', 'SentimentScore']]\n",
    "sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d49d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbols(df, col):\n",
    "    # extract @s\n",
    "    at_users = []\n",
    "    for row in range(len(df)):\n",
    "        matches = re.findall(r'@\\w*\\b', col[row])\n",
    "        if len(matches) == 0:\n",
    "            users = 'NaN'\n",
    "        else:\n",
    "            users = [user[1:] for user in matches]\n",
    "        at_users += [users]\n",
    "    \n",
    "    # extract #s\n",
    "    hashtags = []\n",
    "    for row in range(len(df)):\n",
    "        matches = re.findall(r'#\\w*\\b', col[row])\n",
    "        if len(matches) == 0:\n",
    "            tags = 'NaN'\n",
    "        tags = [tag[1:] for tag in matches ]\n",
    "        hashtags += [tags]\n",
    "    \n",
    "    # extract links\n",
    "    web_links = []\n",
    "    for row in range(len(df)):\n",
    "        matches = re.findall(r'http://\\S+|https://\\S+', col[row])\n",
    "        if len(matches) == 0:\n",
    "            links = 'NaN'\n",
    "        links = [link for link in matches ]\n",
    "        web_links += [links]\n",
    "        \n",
    "    # extract emojis\n",
    "    emoji_list = []\n",
    "    for row in range(len(df)):\n",
    "        matches = []\n",
    "        temp = list(col[row])\n",
    "        for ch in temp:\n",
    "            if (emoji.is_emoji(ch)):\n",
    "                matches += [ch]\n",
    "        if len(matches) == 0:\n",
    "             emoji_list += [\"NaN\"]\n",
    "        else:\n",
    "            emoji_list += [matches]\n",
    "    \n",
    "    return at_users, hashtags, web_links, emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc20641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform label extraction on sentiments\n",
    "text = sent_df['text'].astype('string').fillna('NA')\n",
    "user, hashtags, web_links, emoji_list = extract_symbols(sent_df, text)\n",
    "# store in new columns\n",
    "sent_df[\"com_at_users\"] = user\n",
    "sent_df[\"com_hashtags\"] = hashtags\n",
    "sent_df[\"com_web_links\"] = web_links\n",
    "sent_df[\"com_emoji_list\"] = emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2e5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stopwords = stopwords.words('english')\n",
    "additional_stopwords = ['none']\n",
    "original_stopwords.extend(additional_stopwords)\n",
    "stopwords = set(original_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    if type(text) == np.float:\n",
    "        return \"\"\n",
    "    temp = text.lower() # to lower case\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp) # remove @s\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp) # remove hashtags\n",
    "    temp = re.sub(r'http\\S+', '', temp) # remove links\n",
    "    temp = re.sub(r\"www.\\S+\", \"\", temp) # remove links\n",
    "    temp = re.sub(r'\\n|[^a-zA-Z]', ' ', temp) # remove punctuation\n",
    "    temp = temp.replace(\"\\n\", \" \").split()\n",
    "    temp = [w for w in temp if not w in stopwords] # remove stopwords\n",
    "    temp = [w for w in temp if not w.isdigit()] # remove numbers\n",
    "    temp = [unidecode.unidecode(w) for w in temp] # turn non-enlish letters to english letters\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0bdf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df[\"text_cleaned\"] = [clean_text(t) for t in sent_df[\"text\"]]\n",
    "sent_df['lemmatized_text'] = sent_df['text'].apply(lambda text: lemmatize_words(text))\n",
    "#tokenization\n",
    "sent_df['tokenized_text'] = sent_df['lemmatized_text'].apply(word_tokenize) \n",
    "# stemming\n",
    "ps = PorterStemmer()\n",
    "sent_df['stemmed_text'] = sent_df['text_cleaned'].apply(lambda x: \" \".join([ps.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4230f7",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd3ae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9022, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sent_df = sent_df[['text_cleaned', 'SentimentScore']]\n",
    "sent_df = sent_df[sent_df['SentimentScore']<5]\n",
    "sent_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3604bc7",
   "metadata": {},
   "source": [
    "#### Train Test Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea43078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7217, 1)\n",
      "(1805, 1)\n",
      "(7217, 1)\n",
      "(1805, 1)\n"
     ]
    }
   ],
   "source": [
    "categories = ['SentimentScore']\n",
    "train, test = train_test_split(sent_df, random_state=42, test_size=0.2, shuffle=True)\n",
    "X_train = train[['text_cleaned']]\n",
    "X_test = test[['text_cleaned']]\n",
    "Y_train = train[['SentimentScore']]\n",
    "Y_test = test[['SentimentScore']]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15412c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tf\", CountVectorizer(stop_words=stop_words), 'text_cleaned'),\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=stop_words), 'text_cleaned')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cfde0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_value = 2\n",
    "model_lst = [RandomForestRegressor(max_depth=10, random_state=random_state_value), #Random Forest\n",
    "             LinearRegression(), #Linear Regression\n",
    "             DecisionTreeRegressor(random_state=random_state_value), #Decision Tree\n",
    "             Ridge(alpha=1.0, random_state=random_state_value), #Ridge\n",
    "             Lasso(alpha=1.0, random_state=random_state_value) #Lasso\n",
    "            ]\n",
    "\n",
    "model_name_lst = ['Random Forest Regressor', 'Linear Regression', 'Decision Tree Regressor', 'Ridge', 'Lasso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7ec784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model, model_name):\n",
    "    pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('regressor', model),\n",
    "            ])\n",
    "\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    prediction = pipeline.predict(pd.DataFrame(X_test))\n",
    "    mse = mean_squared_error(Y_test, prediction)\n",
    "    print('The model name is ' + model_name + '.')\n",
    "    print('MSE is {}'.format(round(mse, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4f573",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8a25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is Random Forest Regressor.\n",
      "MSE is 0.3329\n"
     ]
    }
   ],
   "source": [
    "rd, rd_name = model_lst[0], model_name_lst[0]\n",
    "model_result(rd, rd_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b636f",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d3b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is Linear Regression.\n",
      "MSE is 1.1612\n"
     ]
    }
   ],
   "source": [
    "lr, lr_name = model_lst[1], model_name_lst[1]\n",
    "model_result(lr, lr_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02f593",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04696b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is Decision Tree Regressor.\n",
      "MSE is 0.5005\n"
     ]
    }
   ],
   "source": [
    "dt, dt_name = model_lst[2], model_name_lst[2]\n",
    "model_result(dt, dt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063c901",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e472ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is Ridge.\n",
      "MSE is 0.4073\n"
     ]
    }
   ],
   "source": [
    "ridge, ridge_name = model_lst[3], model_name_lst[3]\n",
    "model_result(ridge, ridge_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54d264",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ce6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is Lasso.\n",
      "MSE is 0.391\n"
     ]
    }
   ],
   "source": [
    "lasso, lasso_name = model_lst[4], model_name_lst[4]\n",
    "model_result(lasso, lasso_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1c6f7",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "288bc79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.265):\n",
      "{'random_forest__max_depth': 1000, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 200, 'random_forest__warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "RFR_pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('random_forest', RandomForestRegressor(random_state=2)),\n",
    "            ])\n",
    "\n",
    "param_grid = {\n",
    "    \"random_forest__n_estimators\": [10, 100, 200],\n",
    "    \"random_forest__max_depth\": [1000, None],\n",
    "    \"random_forest__min_samples_split\": [1, 2],\n",
    "    \"random_forest__min_samples_leaf\": [1, 2],\n",
    "    \"random_forest__min_weight_fraction_leaf\": [0, 1],\n",
    "    \"random_forest__warm_start\": [True, False],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(RFR_pipeline, param_grid, n_jobs=5)\n",
    "search.fit(X_train, Y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cebab195",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('random_forest', RandomForestRegressor(max_depth=1000, \n",
    "                                                        min_samples_leaf=1, \n",
    "                                                        min_samples_split=2, \n",
    "                                                        min_weight_fraction_leaf=0, \n",
    "                                                        n_estimators=200, \n",
    "                                                        warm_start=True,\n",
    "                                                        random_state=2)),\n",
    "            ])\n",
    "\n",
    "RFR_pipeline.fit(X_train, Y_train)\n",
    "prediction = RFR_pipeline.predict(pd.DataFrame(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03723b",
   "metadata": {},
   "source": [
    "### Best Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e64d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.291\n",
      "R2 is 0.253\n"
     ]
    }
   ],
   "source": [
    "mse = round(mean_squared_error(Y_test, prediction), 3)\n",
    "print('MSE is {}'.format(mse))\n",
    "r2 = round(r2_score(Y_test, prediction), 3)\n",
    "print('R2 is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3771f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
